{"cells":[{"cell_type":"code","execution_count":43,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-28T16:23:20.642247Z","iopub.status.busy":"2024-01-28T16:23:20.641674Z","iopub.status.idle":"2024-01-28T16:23:20.664589Z","shell.execute_reply":"2024-01-28T16:23:20.663631Z","shell.execute_reply.started":"2024-01-28T16:23:20.642211Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/simple-dialogs-for-chatbot/dialogs.txt\n"]}],"source":["import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"markdown","metadata":{},"source":["# Introduction:\n","\n","The provided code implements a simple chatbot using TensorFlow and Keras. The chatbot is built on a sequence-to-sequence model with attention mechanisms, allowing it to generate contextually relevant responses based on user input."]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:20.666343Z","iopub.status.busy":"2024-01-28T16:23:20.666047Z","iopub.status.idle":"2024-01-28T16:23:20.671929Z","shell.execute_reply":"2024-01-28T16:23:20.671001Z","shell.execute_reply.started":"2024-01-28T16:23:20.666318Z"},"trusted":true},"outputs":[],"source":["# importing necessary libraries\n","import nltk\n","import numpy as np\n","import random\n","import string\n","import warnings\n","warnings.filterwarnings('ignore')\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import keras \n","from tqdm import tqdm\n","from keras.layers import Dense\n","import json \n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import unicodedata\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{},"source":["# Data loading"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:20.673491Z","iopub.status.busy":"2024-01-28T16:23:20.673119Z","iopub.status.idle":"2024-01-28T16:23:20.687968Z","shell.execute_reply":"2024-01-28T16:23:20.687107Z","shell.execute_reply.started":"2024-01-28T16:23:20.673455Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["question  =[]\n","answer = []\n","with open(\"../input/simple-dialogs-for-chatbot/dialogs.txt\",'r') as f :\n","    for line in f :\n","        line  =  line.split('\\t')\n","        question.append(line[0])\n","        answer.append(line[1].replace(\"\\n\", \"\"))\n","\n","print(len(question) == len(answer))"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:20.690126Z","iopub.status.busy":"2024-01-28T16:23:20.689857Z","iopub.status.idle":"2024-01-28T16:23:20.695569Z","shell.execute_reply":"2024-01-28T16:23:20.694605Z","shell.execute_reply.started":"2024-01-28T16:23:20.690103Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['hi, how are you doing?',\n"," \"i'm fine. how about yourself?\",\n"," \"i'm pretty good. thanks for asking.\",\n"," 'no problem. so how have you been?',\n"," \"i've been great. what about you?\"]"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["question[:5]"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:20.696927Z","iopub.status.busy":"2024-01-28T16:23:20.696604Z","iopub.status.idle":"2024-01-28T16:23:20.705390Z","shell.execute_reply":"2024-01-28T16:23:20.704462Z","shell.execute_reply.started":"2024-01-28T16:23:20.696896Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[\"i'm fine. how about yourself?\",\n"," \"i'm pretty good. thanks for asking.\",\n"," 'no problem. so how have you been?',\n"," \"i've been great. what about you?\",\n"," \"i've been good. i'm in school right now.\"]"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["answer[:5]"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preprocessing:"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:20.707347Z","iopub.status.busy":"2024-01-28T16:23:20.706484Z","iopub.status.idle":"2024-01-28T16:23:20.721683Z","shell.execute_reply":"2024-01-28T16:23:20.720660Z","shell.execute_reply.started":"2024-01-28T16:23:20.707321Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hi, how are you doing?</td>\n","      <td>i'm fine. how about yourself?</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>i'm fine. how about yourself?</td>\n","      <td>i'm pretty good. thanks for asking.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>i'm pretty good. thanks for asking.</td>\n","      <td>no problem. so how have you been?</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>no problem. so how have you been?</td>\n","      <td>i've been great. what about you?</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>i've been great. what about you?</td>\n","      <td>i've been good. i'm in school right now.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                              question  \\\n","0               hi, how are you doing?   \n","1        i'm fine. how about yourself?   \n","2  i'm pretty good. thanks for asking.   \n","3    no problem. so how have you been?   \n","4     i've been great. what about you?   \n","\n","                                     answer  \n","0             i'm fine. how about yourself?  \n","1       i'm pretty good. thanks for asking.  \n","2         no problem. so how have you been?  \n","3          i've been great. what about you?  \n","4  i've been good. i'm in school right now.  "]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["data=pd.DataFrame({\"question\":question,\"answer\":answer})\n","data.head()"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:20.723167Z","iopub.status.busy":"2024-01-28T16:23:20.722827Z","iopub.status.idle":"2024-01-28T16:23:20.730787Z","shell.execute_reply":"2024-01-28T16:23:20.729989Z","shell.execute_reply.started":"2024-01-28T16:23:20.723141Z"},"trusted":true},"outputs":[],"source":["def unicode_to_ascii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s)\n","      if unicodedata.category(c) != 'Mn')"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:20.732138Z","iopub.status.busy":"2024-01-28T16:23:20.731849Z","iopub.status.idle":"2024-01-28T16:23:20.743027Z","shell.execute_reply":"2024-01-28T16:23:20.742326Z","shell.execute_reply.started":"2024-01-28T16:23:20.732113Z"},"trusted":true},"outputs":[],"source":["def clean_text(text):\n","    text = unicode_to_ascii(text.lower().strip())\n","    text = re.sub(r\"i'm\", \"i am\", text)\n","    text = re.sub(r\"\\r\", \"\", text)\n","    text = re.sub(r\"he's\", \"he is\", text)\n","    text = re.sub(r\"she's\", \"she is\", text)\n","    text = re.sub(r\"it's\", \"it is\", text)\n","    text = re.sub(r\"that's\", \"that is\", text)\n","    text = re.sub(r\"what's\", \"that is\", text)\n","    text = re.sub(r\"where's\", \"where is\", text)\n","    text = re.sub(r\"how's\", \"how is\", text)\n","    text = re.sub(r\"\\'ll\", \" will\", text)\n","    text = re.sub(r\"\\'ve\", \" have\", text)\n","    text = re.sub(r\"\\'re\", \" are\", text)\n","    text = re.sub(r\"\\'d\", \" would\", text)\n","    text = re.sub(r\"\\'re\", \" are\", text)\n","    text = re.sub(r\"won't\", \"will not\", text)\n","    text = re.sub(r\"can't\", \"cannot\", text)\n","    text = re.sub(r\"n't\", \" not\", text)\n","    text = re.sub(r\"n'\", \"ng\", text)\n","    text = re.sub(r\"'bout\", \"about\", text)\n","    text = re.sub(r\"'til\", \"until\", text)\n","    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n","    text = text.translate(str.maketrans('', '', string.punctuation)) \n","    text = re.sub(\"(\\\\W)\",\" \",text) \n","    text = re.sub('\\S*\\d\\S*\\s*','', text)\n","    text =  \"<sos> \" +  text + \" <eos>\"\n","    return text"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:20.762534Z","iopub.status.busy":"2024-01-28T16:23:20.762161Z","iopub.status.idle":"2024-01-28T16:23:20.769316Z","shell.execute_reply":"2024-01-28T16:23:20.768357Z","shell.execute_reply.started":"2024-01-28T16:23:20.762502Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0                 hi, how are you doing?\n","1          i'm fine. how about yourself?\n","2    i'm pretty good. thanks for asking.\n","3      no problem. so how have you been?\n","4       i've been great. what about you?\n","Name: question, dtype: object"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["data[\"question\"][:5]"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:20.771299Z","iopub.status.busy":"2024-01-28T16:23:20.770995Z","iopub.status.idle":"2024-01-28T16:23:20.972597Z","shell.execute_reply":"2024-01-28T16:23:20.971917Z","shell.execute_reply.started":"2024-01-28T16:23:20.771275Z"},"trusted":true},"outputs":[],"source":["data[\"question\"]=data.question.apply(clean_text)"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:20.973696Z","iopub.status.busy":"2024-01-28T16:23:20.973471Z","iopub.status.idle":"2024-01-28T16:23:20.980036Z","shell.execute_reply":"2024-01-28T16:23:20.979061Z","shell.execute_reply.started":"2024-01-28T16:23:20.973676Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0                  <sos> hi how are you doing <eos>\n","1          <sos> i am fine how about yourself <eos>\n","2    <sos> i am pretty good thanks for asking <eos>\n","3       <sos> no problem so how have you been <eos>\n","4      <sos> i have been great what about you <eos>\n","Name: question, dtype: object"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["data[\"question\"][:5]"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:20.981786Z","iopub.status.busy":"2024-01-28T16:23:20.981079Z","iopub.status.idle":"2024-01-28T16:23:21.185937Z","shell.execute_reply":"2024-01-28T16:23:21.185130Z","shell.execute_reply.started":"2024-01-28T16:23:20.981762Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0             <sos> i am fine how about yourself <eos>\n","1       <sos> i am pretty good thanks for asking <eos>\n","2          <sos> no problem so how have you been <eos>\n","3         <sos> i have been great what about you <eos>\n","4    <sos> i have been good i am in school right no...\n","Name: answer, dtype: object"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["data[\"answer\"] = data.answer.apply(clean_text)\n","data[\"answer\"][:5]"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:21.188478Z","iopub.status.busy":"2024-01-28T16:23:21.188215Z","iopub.status.idle":"2024-01-28T16:23:21.193766Z","shell.execute_reply":"2024-01-28T16:23:21.192882Z","shell.execute_reply.started":"2024-01-28T16:23:21.188455Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['<sos> hi how are you doing <eos>', '<sos> i am fine how about yourself <eos>', '<sos> i am pretty good thanks for asking <eos>', '<sos> no problem so how have you been <eos>', '<sos> i have been great what about you <eos>']\n","['<sos> i am fine how about yourself <eos>', '<sos> i am pretty good thanks for asking <eos>', '<sos> no problem so how have you been <eos>', '<sos> i have been great what about you <eos>', '<sos> i have been good i am in school right now <eos>']\n"]}],"source":["question  = data.question.values.tolist()\n","answer =  data.answer.values.tolist()\n","print(question[:5])  # Print the first 5 questions\n","print(answer[:5]) "]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:21.195226Z","iopub.status.busy":"2024-01-28T16:23:21.194898Z","iopub.status.idle":"2024-01-28T16:23:21.202919Z","shell.execute_reply":"2024-01-28T16:23:21.202175Z","shell.execute_reply.started":"2024-01-28T16:23:21.195172Z"},"trusted":true},"outputs":[],"source":["def tokenize(lang):\n","    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","      filters='')\n","    lang_tokenizer.fit_on_texts(lang)\n","    tensor = lang_tokenizer.texts_to_sequences(lang)\n","\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n","                                                         padding='post')\n","\n","    return tensor, lang_tokenizer"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:21.204310Z","iopub.status.busy":"2024-01-28T16:23:21.203994Z","iopub.status.idle":"2024-01-28T16:23:21.393806Z","shell.execute_reply":"2024-01-28T16:23:21.393031Z","shell.execute_reply.started":"2024-01-28T16:23:21.204287Z"},"trusted":true},"outputs":[],"source":["input_tensor, inp_lang = tokenize(question)\n","target_tensor, targ_lang = tokenize(answer)\n"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:37:46.139386Z","iopub.status.busy":"2024-01-28T16:37:46.138490Z","iopub.status.idle":"2024-01-28T16:37:46.143798Z","shell.execute_reply":"2024-01-28T16:37:46.142791Z","shell.execute_reply.started":"2024-01-28T16:37:46.139352Z"},"trusted":true},"outputs":[],"source":["def remove_tags(sentence):\n","    return sentence.split(\"<start>\")[-1].split(\"<end>\")[0]"]},{"cell_type":"code","execution_count":84,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:38:09.685289Z","iopub.status.busy":"2024-01-28T16:38:09.684761Z","iopub.status.idle":"2024-01-28T16:38:09.689938Z","shell.execute_reply":"2024-01-28T16:38:09.688985Z","shell.execute_reply.started":"2024-01-28T16:38:09.685252Z"},"trusted":true},"outputs":[],"source":["max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"]},{"cell_type":"markdown","metadata":{},"source":["* The chatbot is trained on a dataset containing pairs of questions and answers.\n","* The clean_text function handles text preprocessing, including lowercasing, replacing contractions, removing special characters, and adding start and end tokens (<sos> and <eos>)."]},{"cell_type":"markdown","metadata":{},"source":["# Model Architecture:"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:21.415284Z","iopub.status.busy":"2024-01-28T16:23:21.414908Z","iopub.status.idle":"2024-01-28T16:23:21.424726Z","shell.execute_reply":"2024-01-28T16:23:21.423930Z","shell.execute_reply.started":"2024-01-28T16:23:21.415259Z"},"trusted":true},"outputs":[],"source":["## Creating training and validation sets using an 80-20 split\n","input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:21.426089Z","iopub.status.busy":"2024-01-28T16:23:21.425838Z","iopub.status.idle":"2024-01-28T16:23:21.435217Z","shell.execute_reply":"2024-01-28T16:23:21.434372Z","shell.execute_reply.started":"2024-01-28T16:23:21.426067Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2980 745 2980 745\n"]}],"source":["print(len(input_tensor_train) , len(input_tensor_val) , len(target_tensor_train) , len(target_tensor_val))"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:21.436637Z","iopub.status.busy":"2024-01-28T16:23:21.436256Z","iopub.status.idle":"2024-01-28T16:23:21.461206Z","shell.execute_reply":"2024-01-28T16:23:21.460372Z","shell.execute_reply.started":"2024-01-28T16:23:21.436604Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(TensorShape([64, 22]), TensorShape([64, 22]))"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["BUFFER_SIZE = len(input_tensor_train)\n","BATCH_SIZE = 64\n","steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n","embedding_dim = 256\n","units = 1024\n","vocab_inp_size = len(inp_lang.word_index)+1\n","vocab_tar_size = len(targ_lang.word_index)+1\n","\n","dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","\n","example_input_batch, example_target_batch = next(iter(dataset))\n","example_input_batch.shape, example_target_batch.shape"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:21.462521Z","iopub.status.busy":"2024-01-28T16:23:21.462240Z","iopub.status.idle":"2024-01-28T16:23:21.469600Z","shell.execute_reply":"2024-01-28T16:23:21.468700Z","shell.execute_reply.started":"2024-01-28T16:23:21.462497Z"},"trusted":true},"outputs":[],"source":["class Encoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n","        super(Encoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.enc_units = enc_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = tf.keras.layers.GRU(self.enc_units,\n","                                       return_sequences=True,\n","                                       return_state=True,\n","                                       recurrent_initializer='glorot_uniform')\n","\n","    def call(self, x,hidden):\n","        x = self.embedding(x)\n","        output, state = self.gru(x, initial_state = hidden)\n","        return output, state\n","    \n","    def initialize_hidden_state(self):\n","        return tf.zeros((self.batch_sz, self.enc_units))"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:21.473761Z","iopub.status.busy":"2024-01-28T16:23:21.473505Z","iopub.status.idle":"2024-01-28T16:23:21.502133Z","shell.execute_reply":"2024-01-28T16:23:21.501433Z","shell.execute_reply.started":"2024-01-28T16:23:21.473739Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoder output shape: (batch size, sequence length, units) (64, 22, 1024)\n","Encoder Hidden state shape: (batch size, units) (64, 1024)\n"]}],"source":["encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n","\n","# sample input\n","sample_hidden = encoder.initialize_hidden_state()\n","sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n","print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n","print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:21.503320Z","iopub.status.busy":"2024-01-28T16:23:21.503052Z","iopub.status.idle":"2024-01-28T16:23:21.510749Z","shell.execute_reply":"2024-01-28T16:23:21.509879Z","shell.execute_reply.started":"2024-01-28T16:23:21.503298Z"},"trusted":true},"outputs":[],"source":["class BahdanauAttention(tf.keras.layers.Layer):\n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","        self.W1 = tf.keras.layers.Dense(units)\n","        self.W2 = tf.keras.layers.Dense(units)\n","        self.V = tf.keras.layers.Dense(1)\n","\n","    def call(self, query, values):\n","        # query hidden state shape == (batch_size, hidden size)\n","        # query_with_time_axis shape == (batch_size, 1, hidden size)\n","        # values shape == (batch_size, max_len, hidden size)\n","        # we are doing this to broadcast addition along the time axis to calculate the score\n","        query_with_time_axis = tf.expand_dims(query, 1)\n","\n","        # score shape == (batch_size, max_length, 1)\n","        # we get 1 at the last axis because we are applying score to self.V\n","        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n","        score = self.V(tf.nn.tanh(\n","            self.W1(query_with_time_axis) + self.W2(values)))\n","\n","        # attention_weights shape == (batch_size, max_length, 1)\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","\n","        # context_vector shape after sum == (batch_size, hidden_size)\n","        context_vector = attention_weights * values\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","        return context_vector, attention_weights"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:21.512750Z","iopub.status.busy":"2024-01-28T16:23:21.512049Z","iopub.status.idle":"2024-01-28T16:23:21.539925Z","shell.execute_reply":"2024-01-28T16:23:21.539120Z","shell.execute_reply.started":"2024-01-28T16:23:21.512718Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Attention result shape: (batch size, units) (64, 1024)\n","Attention weights shape: (batch_size, sequence_length, 1) (64, 22, 1)\n"]}],"source":["attention_layer = BahdanauAttention(10)\n","attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n","\n","print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n","print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:21.541035Z","iopub.status.busy":"2024-01-28T16:23:21.540810Z","iopub.status.idle":"2024-01-28T16:23:21.550363Z","shell.execute_reply":"2024-01-28T16:23:21.549388Z","shell.execute_reply.started":"2024-01-28T16:23:21.541016Z"},"trusted":true},"outputs":[],"source":["class Decoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n","        super(Decoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.dec_units = dec_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = tf.keras.layers.GRU(self.dec_units,\n","                                       return_sequences=True,\n","                                       return_state=True,\n","                                       recurrent_initializer='glorot_uniform')\n","        self.fc = tf.keras.layers.Dense(vocab_size)\n","\n","        # used for attention\n","        self.attention = BahdanauAttention(self.dec_units)\n","\n","    def call(self, x, hidden, enc_output):\n","        # enc_output shape == (batch_size, max_length, hidden_size)\n","        context_vector, attention_weights = self.attention(hidden, enc_output)\n","\n","        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","        x = self.embedding(x)\n","\n","        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","        # passing the concatenated vector to the GRU\n","        output, state = self.gru(x)\n","\n","        # output shape == (batch_size * 1, hidden_size)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","\n","        # output shape == (batch_size, vocab)\n","        x = self.fc(output)\n","\n","        return x, state, attention_weights"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:21.551581Z","iopub.status.busy":"2024-01-28T16:23:21.551334Z","iopub.status.idle":"2024-01-28T16:23:21.604080Z","shell.execute_reply":"2024-01-28T16:23:21.603275Z","shell.execute_reply.started":"2024-01-28T16:23:21.551559Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Decoder output shape: (batch_size, vocab size) (64, 2347)\n"]}],"source":["decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n","\n","sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n","                                      sample_hidden, sample_output)\n","\n","print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:21.605634Z","iopub.status.busy":"2024-01-28T16:23:21.605368Z","iopub.status.idle":"2024-01-28T16:23:21.612946Z","shell.execute_reply":"2024-01-28T16:23:21.612087Z","shell.execute_reply.started":"2024-01-28T16:23:21.605612Z"},"trusted":true},"outputs":[],"source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_)"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:21.614276Z","iopub.status.busy":"2024-01-28T16:23:21.614005Z","iopub.status.idle":"2024-01-28T16:23:21.925591Z","shell.execute_reply":"2024-01-28T16:23:21.924726Z","shell.execute_reply.started":"2024-01-28T16:23:21.614254Z"},"trusted":true},"outputs":[],"source":["@tf.function\n","def train_step(inp, targ, enc_hidden):\n","    loss = 0\n","\n","    with tf.GradientTape() as tape:\n","        enc_output, enc_hidden = encoder(inp, enc_hidden)\n","\n","        dec_hidden = enc_hidden\n","\n","        dec_input = tf.expand_dims([targ_lang.word_index['<sos>']] * BATCH_SIZE, 1)\n","\n","        # Teacher forcing - feeding the target as the next input\n","        for t in range(1, targ.shape[1]):\n","            # passing enc_output to the decoder\n","            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","\n","            loss += loss_function(targ[:, t], predictions)\n","\n","            # using teacher forcing\n","            dec_input = tf.expand_dims(targ[:, t], 1)\n","\n","    batch_loss = (loss / int(targ.shape[1]))\n","\n","    variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","    gradients = tape.gradient(loss, variables)\n","\n","    optimizer.apply_gradients(zip(gradients, variables))\n","\n","    return batch_loss"]},{"cell_type":"markdown","metadata":{},"source":["* The model consists of an Encoder and a Decoder, both implemented as TensorFlow Keras models.\n","* The Encoder utilizes an embedding layer and a GRU (Gated Recurrent Unit) layer to process input sequences.\n","* The Bahdanau Attention mechanism is incorporated into the Decoder to focus on different parts of the input sequence when generating output.\n","* The model is trained using the Adam optimizer and Sparse Categorical Crossentropy loss."]},{"cell_type":"markdown","metadata":{},"source":["# Training:"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:23:21.929176Z","iopub.status.busy":"2024-01-28T16:23:21.928880Z","iopub.status.idle":"2024-01-28T16:28:32.896297Z","shell.execute_reply":"2024-01-28T16:28:32.895309Z","shell.execute_reply.started":"2024-01-28T16:23:21.929152Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Epochs:  10%|█         | 4/40 [01:02<07:15, 12.09s/epoch]"]},{"name":"stdout","output_type":"stream","text":["Epoch:  4 Loss:1.5774\n"]},{"name":"stderr","output_type":"stream","text":["Epochs:  20%|██        | 8/40 [01:31<04:20,  8.16s/epoch]"]},{"name":"stdout","output_type":"stream","text":["Epoch:  8 Loss:1.3481\n"]},{"name":"stderr","output_type":"stream","text":["Epochs:  30%|███       | 12/40 [01:59<03:22,  7.23s/epoch]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 12 Loss:1.1658\n"]},{"name":"stderr","output_type":"stream","text":["Epochs:  40%|████      | 16/40 [02:26<02:48,  7.00s/epoch]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 16 Loss:0.9982\n"]},{"name":"stderr","output_type":"stream","text":["Epochs:  50%|█████     | 20/40 [02:54<02:17,  6.86s/epoch]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 20 Loss:0.8138\n"]},{"name":"stderr","output_type":"stream","text":["Epochs:  60%|██████    | 24/40 [03:21<01:50,  6.93s/epoch]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 24 Loss:0.6226\n"]},{"name":"stderr","output_type":"stream","text":["Epochs:  70%|███████   | 28/40 [03:48<01:22,  6.88s/epoch]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 28 Loss:0.4128\n"]},{"name":"stderr","output_type":"stream","text":["Epochs:  80%|████████  | 32/40 [04:16<00:55,  6.93s/epoch]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 32 Loss:0.2315\n"]},{"name":"stderr","output_type":"stream","text":["Epochs:  90%|█████████ | 36/40 [04:43<00:27,  6.87s/epoch]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 36 Loss:0.1043\n"]},{"name":"stderr","output_type":"stream","text":["Epochs: 100%|██████████| 40/40 [05:10<00:00,  7.77s/epoch]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 40 Loss:0.0541\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["EPOCHS = 40\n","\n","for epoch in tqdm(range(1, EPOCHS + 1), desc='Epochs', unit='epoch'):\n","    enc_hidden = encoder.initialize_hidden_state()\n","    total_loss = 0\n","\n","    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","        batch_loss = train_step(inp, targ, enc_hidden)\n","        total_loss += batch_loss\n","\n","    if epoch % 4 == 0:\n","        print('Epoch:{:3d} Loss:{:.4f}'.format(epoch, total_loss / steps_per_epoch))"]},{"cell_type":"markdown","metadata":{},"source":["* The training loop (train_step) trains the model on batches of input-output pairs.\n","* The training process involves encoding the input sequence, decoding it to generate predictions, and calculating the loss.\n","* The model is trained for a specified number of epochs."]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation:"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:28:32.897828Z","iopub.status.busy":"2024-01-28T16:28:32.897534Z","iopub.status.idle":"2024-01-28T16:28:32.906883Z","shell.execute_reply":"2024-01-28T16:28:32.905965Z","shell.execute_reply.started":"2024-01-28T16:28:32.897803Z"},"trusted":true},"outputs":[],"source":["def evaluate(sentence):\n","    sentence = clean_text(sentence)\n","\n","    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                         maxlen=max_length_inp,\n","                                                         padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","\n","    result = ''\n","\n","    hidden = [tf.zeros((1, units))]\n","    enc_out, enc_hidden = encoder(inputs, hidden)\n","\n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([targ_lang.word_index['<sos>']], 0)\n","\n","    for t in range(max_length_targ):\n","        predictions, dec_hidden, attention_weights = decoder(dec_input,\n","                                                             dec_hidden,\n","                                                             enc_out)\n","\n","        # storing the attention weights to plot later on\n","        attention_weights = tf.reshape(attention_weights, (-1, ))\n","\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","        result += targ_lang.index_word[predicted_id] + ' '\n","\n","        if targ_lang.index_word[predicted_id] == '<eos>':\n","            return remove_tags(result), remove_tags(sentence)\n","\n","        # the predicted ID is fed back into the model\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","    return remove_tags(result), remove_tags(sentence)"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:28:32.908248Z","iopub.status.busy":"2024-01-28T16:28:32.907873Z","iopub.status.idle":"2024-01-28T16:28:32.928896Z","shell.execute_reply":"2024-01-28T16:28:32.928001Z","shell.execute_reply.started":"2024-01-28T16:28:32.908217Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["questions  =[]\n","answers = []\n","with open(\"../input/simple-dialogs-for-chatbot/dialogs.txt\",'r') as f :\n","    for line in f :\n","        line  =  line.split('\\t')\n","        questions.append(line[0])\n","        answers.append(line[1])\n","print(len(question) == len(answer))"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:28:32.930750Z","iopub.status.busy":"2024-01-28T16:28:32.930145Z","iopub.status.idle":"2024-01-28T16:28:33.091975Z","shell.execute_reply":"2024-01-28T16:28:33.091124Z","shell.execute_reply.started":"2024-01-28T16:28:32.930717Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Question: <sos> i believe so <eos>\n","Predicted answer: good i hope it does not cool off this weekend <eos> \n"]}],"source":["def ask(sentence):\n","    result, sentence = evaluate(sentence)\n","\n","    print('Question: %s' % (sentence))\n","    print('Predicted answer: {}'.format(result))\n","ask(questions[100])"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:28:33.093270Z","iopub.status.busy":"2024-01-28T16:28:33.092980Z","iopub.status.idle":"2024-01-28T16:28:33.252267Z","shell.execute_reply":"2024-01-28T16:28:33.251298Z","shell.execute_reply.started":"2024-01-28T16:28:33.093246Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Question: <sos> it is not bad there are a lot of people there <eos>\n","Predicted answer: there are a lot of poor people in be lucky <eos> \n"]}],"source":["ask(questions[20])"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:28:33.253924Z","iopub.status.busy":"2024-01-28T16:28:33.253545Z","iopub.status.idle":"2024-01-28T16:28:33.259124Z","shell.execute_reply":"2024-01-28T16:28:33.258210Z","shell.execute_reply.started":"2024-01-28T16:28:33.253886Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["good luck with that.\n","\n"]}],"source":["print(answers[20])"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:28:33.260996Z","iopub.status.busy":"2024-01-28T16:28:33.260396Z","iopub.status.idle":"2024-01-28T16:28:33.366768Z","shell.execute_reply":"2024-01-28T16:28:33.365904Z","shell.execute_reply.started":"2024-01-28T16:28:33.260962Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Question: <sos> i have actually been pretty good you <eos>\n","Predicted answer: i am actually in school tomorrow <eos> \n"]}],"source":["ask(questions[15])"]},{"cell_type":"code","execution_count":78,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:28:33.367981Z","iopub.status.busy":"2024-01-28T16:28:33.367705Z","iopub.status.idle":"2024-01-28T16:28:33.372683Z","shell.execute_reply":"2024-01-28T16:28:33.371752Z","shell.execute_reply.started":"2024-01-28T16:28:33.367950Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["thank you very much.\n","\n"]}],"source":["print(answers[10])"]},{"cell_type":"markdown","metadata":{},"source":["* The evaluate function processes user input, tokenizes it, and feeds it through the trained model to generate a response.\n","* Attention weights are visualized for each input"]},{"cell_type":"markdown","metadata":{},"source":["# Chatbot Interaction:Handling Out-of-Vocabulary Words"]},{"cell_type":"code","execution_count":87,"metadata":{"execution":{"iopub.execute_input":"2024-01-28T16:40:03.242066Z","iopub.status.busy":"2024-01-28T16:40:03.241301Z","iopub.status.idle":"2024-01-28T16:40:34.004065Z","shell.execute_reply":"2024-01-28T16:40:34.003105Z","shell.execute_reply.started":"2024-01-28T16:40:03.242034Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Chatbot: Hi! I'm your chatbot. Type 'exit' to end the conversation.\n"]},{"name":"stdout","output_type":"stream","text":["You:  hi\n"]},{"name":"stdout","output_type":"stream","text":["User: <sos> hi <eos>\n","Chatbot: well i think i am worried about you <eos> \n"]},{"name":"stdout","output_type":"stream","text":["You:  how have you been\n"]},{"name":"stdout","output_type":"stream","text":["User: <sos> how have you been <eos>\n","Chatbot: i have not been pretty good you <eos> \n"]},{"name":"stdout","output_type":"stream","text":["You:  i am fine\n"]},{"name":"stdout","output_type":"stream","text":["User: <sos> i am fine <eos>\n","Chatbot: well listen day <eos> \n"]},{"name":"stdout","output_type":"stream","text":["You:  exit\n"]},{"name":"stdout","output_type":"stream","text":["Chatbot: Goodbye! Have a great day.\n"]}],"source":["def evaluate(sentence):\n","    sentence = clean_text(sentence)\n","\n","    # Create a list of valid inputs, excluding words not in the vocabulary\n","    inputs = [inp_lang.word_index[i] for i in sentence.split(' ') if i in inp_lang.word_index]\n","\n","    if not inputs:\n","        print(\"Chatbot: Sorry, I didn't understand that.\")\n","        return None, None\n","\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                         maxlen=max_length_inp,\n","                                                         padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","\n","    result = ''\n","\n","    hidden = [tf.zeros((1, units))]\n","    enc_out, enc_hidden = encoder(inputs, hidden)\n","\n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([targ_lang.word_index['<sos>']], 0)\n","\n","    for t in range(max_length_targ):\n","        predictions, dec_hidden, attention_weights = decoder(dec_input,\n","                                                             dec_hidden,\n","                                                             enc_out)\n","\n","        # storing the attention weights to plot later on\n","        attention_weights = tf.reshape(attention_weights, (-1, ))\n","\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","        result += targ_lang.index_word.get(predicted_id, '') + ' '\n","\n","        if targ_lang.index_word.get(predicted_id, '') == '<eos>':\n","            return remove_tags(result), remove_tags(sentence)\n","\n","        # the predicted ID is fed back into the model\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","    return remove_tags(result), remove_tags(sentence)\n","\n","def chatbot():\n","    print(\"Chatbot: Hi! I'm your chatbot. Type 'exit' to end the conversation.\")\n","    \n","    while True:\n","        user_input = input(\"You: \")\n","\n","        if user_input.lower() == 'exit':\n","            print(\"Chatbot: Goodbye! Have a great day.\")\n","            break\n","\n","        response, cleaned_input = evaluate(user_input)\n","        print(f\"User: {cleaned_input}\")\n","        print(f\"Chatbot: {response}\")\n","\n","if __name__ == \"__main__\":\n","    chatbot()\n"]},{"cell_type":"markdown","metadata":{},"source":["* The code includes a check in the evaluate function to handle words not present in the vocabulary.\n","* If a word is not in the vocabulary, the chatbot informs the user and continues the conversation.\n","* Run the chatbot function to interact with the trained chatbot.\n","* Type messages and receive responses until choosing to exit."]},{"cell_type":"markdown","metadata":{},"source":["# Conclusion:\n","\n","This chatbot demonstrates a basic conversational agent capable of generating responses based on a trained sequence-to-sequence model. Further enhancements, such as incorporating a larger and more diverse dataset, fine-tuning model parameters, and implementing more advanced architectures, can be explored for improved performance and natural language understanding."]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":715041,"sourceId":1245709,"sourceType":"datasetVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
